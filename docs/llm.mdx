---
title: 'LLMClient'
icon: 'microchip-ai'
---

### LLM Client object configuration
The `LLMClient` object from `superagentx.llm` is a configuration-driven interface to interact with large language models (LLMs) from various
providers, such as OpenAI, AWSBedrock and more. It allows users to set parameters specific to the model and
provider they intend to use, making it easier to configure different types of LLMs and interact with it.

### Supported LLMs

| Icon                                                                                                                                                                                         | LLM Name          |
|:-----------------------------------------------------------------------------------------------------------------------------------------:                                                   | :---------------  |
| <a href="#deepseek"><img src="/images/llms/deepseek.png" title="deepseek" height="25" width="25" style={{ margin: 0, width: 25}} /></a>                  | **DeepSeek**      |
| <a href="#openai"><img src="/images/llms/openai.png" title="OpenAI" height="25" width="25" style={{ margin: 0, width: 25 }} /></a>                      | **OpenAI**        |
| <a href="#azure openai"><img src="/images/llms/azure-icon.png" title="Azure OpenAI" height="25" width="25" style={{ margin: 0, width: 25 }} /></a>            | **Azure OpenAI**  |
| <a href="#aws bedrock"><img src="/images/llms/awsbedrock.png" title="AWS Bedrock" height="25" width="25" style={{ margin: 0, width: 25 }} /></a>            | **AWS Bedrock**   |
| <a href="#google gemini"><img src="/images/llms/gemini.png" title="Google Gemini" height="25" width="25" style={{ margin: 0, width: 25 }} /></a>               | **Google Gemini** |
| <a href="#meta llama"><img src="/images/llms/meta.png" title="Meta LLama" height="25" width="25" style={{ margin: 0, width: 25 }} /></a>                 | **Meta Llama**    |
| <a href="#ollama"><img src="/images/llms/ollama.png" title="Ollama" height="15" width="25" style={{ margin: 0, width: 25 }} /></a>                      | **Ollama**        |
| <a href="#claude ai"><img src="/images/llms/claude-ai-logo.png" title="Claude AI" height="25" width="25" style={{ margin: 0, width: 25 }} /></a>           | **Claude AI**     |
| <a href="#mistral ai"><img src="/images/llms/mistral-ai-logo.png" title="Mistral AI" height="25" width="35" style={{ margin: 0, width: 25 }} /></a>         | **Mistral AI**    |
| <a href="#ibm watson ai"><img src="/images/llms/ibm.png" title="IBM WatsonX AI" height="25" width="35" style={{ margin: 0, width: 25 }} /></a>                 | **IBM WatsonX**   |


### Parameters
| Attribute                          | Parameters        | Type        | Description                                                                                                                                                                                                                |
| :--------------------              | :---------------- | :-------    | :-------------------------------------                                                                                                                                                                                     |
| **LLM Type**                       | `llm_type`        | `str`       | This key identifies the service provider or platform from which the language model is sourced. Common values are: ‘openai’: Refers to OpenAI’s GPT models.                                                                 |
| **Model** _(optional)_             | `model`           | `str`       | This specifies the language model you wish to use. The model is typically identified by a name or version, such as ‘gpt-4o’, ‘gpt-3.5-turbo’, or another model version that the service provider supports. Default `None`  |
| **API Key**  _(optional)_          | `api_key`         | `str`       | You can provide the model API key either as a parameter or retrieve it from the environment. Default `None`                                                                                                                |
| **Base Url** _(optional)_          | `base_url`        | `str`       | The base URL for the model API. Default `None`                                                                                                                                                                             |
| **API Version** _(optional)_       | `api_version`     | `str`       | The required API version for the Azure OpenAI llm_type. Default `None`                                                                                                                                                     |
| **Async Mode**  _(optional)_       | `async_mode`      | `bool`      | Asynchronous mode of OpenAI or Azure OpenAI client. Default `None`                                                                                                                                                         |
| **Embedding Model** _(optional)_   | `embed_model`     | `str`       | Embedding model name, supported models openai, azure-openai, mistral, llama 3.1. Default `None`                                                                                                                            |


<span id="deepseek"></span>
```python DeepSeek
from superagentx.llm import LLMClient

llm_config = {
    'model': "deepseek-chat",
    'llm_type': 'deepseek',
    'base_url': 'https://api.deepseek.com/'
}
llm_client: LLMClient = LLMClient(llm_config=llm_config)
```
<span id="openai"></span>
```python OpenAI
from superagentx.llm import LLMClient

llm_config = {
    "model":'gpt-4o',
    "llm_type":'openai'
}
llm_client = LLMClient(llm_config=llm_config)
```
<span id="aws bedrock"></span>
```python AWSBedrock
from superagentx.llm import LLMClient

llm_config = {
    "model":'anthropic.claude-3-5-sonnet-20241022-v2:0',
    "llm_type":'bedrock'
}
llm_client = LLMClient(llm_config=llm_config)
```

<span id="ollama"></span>
```python ollama
from superagentx.llm import LLMClient

llm_config = {
    'model': 'mistral',
    'llm_type': 'ollama',
}
llm_client: LLMClient = LLMClient(llm_config=llm_config)
```

<span id="claude ai"></span>
```python Claude AI
from superagentx.llm import LLMClient

llm_config = {
    'model': 'claude-3-7-sonnet-20250219',
    'llm_type': 'anthropic'
}
llm_client: LLMClient = LLMClient(llm_config=llm_config)
```

### LLM Type and its Values

| Type                               | value                  | Example                                     |
| :--------------------              | :----------------      | :-------------------------------------      |
| **DeepSeek**                       | `deepseek`             | `llm_config = { 'llm_type':'deepseek'}`     |
| **Open AI**                        | `openai`               | `llm_config = { 'llm_type':'openai'}`       |
| **Azure OpenAI**                   | `azure-openai`         | `llm_config = { 'llm_type':'azure-openai'}` |
| **AWS Bedrock**                    | `bedrock`              | `llm_config = { 'llm_type':'bedrock'}`      |
| **Ollama**                         | `ollama`               | `llm_config = { 'llm_type':'ollama'}`
| **LLama**                          | `llama`                | `llm_config = { 'llm_type':'llama'}`        |
| **Gemini**                         | `gemini`               | `llm_config = { 'llm_type':'gemini'}`       |
| **Anthropic**                      | `anthropic`            | `llm_config = { 'llm_type':'anthropic'}`    |
| **Mistral**                        | `mistral`              | `Coming Soon`                               |
| **Groq**                           | `groq`                 | `Coming Soon`                               |
| **Together**                       | `together`             | `Coming Soon`                               |

### OpenAI Supported Models

| Type                               | Example                                                                          |
| :--------------------              | :----------------------------------------------------------                      |
| **gpt-4o**                         | ```llm_config = { "model":'gpt-4o', "llm_type":'openai'}```                      |
| **gpt-4.1**                        | ```llm_config = { "model":'gpt-4.1', "llm_type":'openai'}```                     |
| **gpt-4**                          | ```llm_config = { "model":'gpt-4', "llm_type":'openai'}```                       |
| **gpt-4o-2024-05-13**              | ```llm_config = { "model":'gpt-4o-2024-05-13', "llm_type":'openai'}```           |
| **gpt-4o-2024-08-06**              | ```llm_config = { "model":'gpt-4o-2024-08-06', "llm_type":'openai'}```           |
| **gpt-4-turbo-2024-04-09**         | ```llm_config = { "model":'gpt-4-turbo-2024-04-09', "llm_type":'openai'}```      |
| **gpt-4o-mini-2024-07-18**         | ```llm_config = { "model":'gpt-4o-mini-2024-07-18', "llm_type":'openai'}```      |
| **gpt-4o-mini**                    | ```llm_config = { "model":'gpt-4o-mini', "llm_type":'openai'}```                 |


### Bedrock Supported Models

| Type                                           | Example                                                                                            |
| :--------------------                          | :----------------------------------------------------------                                        |
| **anthropic.claude-3-5-sonnet-20241022-v2:0**  | ```llm_config = { "model":'anthropic.claude-3-5-sonnet-20241022-v2:0', "llm_type":'bedrock'}```    |
| **anthropic.claude-3-5-haiku-20241022-v1:0**   | ```llm_config = { "model":'anthropic.claude-3-5-haiku-20241022-v1:0', "llm_type":'bedrock'}```     |
| **anthropic.claude-instant-v1:2:100k**         | ```llm_config = {"model":'anthropic.claude-instant-v1:2:100k',"llm_type":'bedrock'}```             |
| **anthropic.claude-3-sonnet-20240229-v1:0**    | ```llm_config = {"model":'anthropic.claude-3-sonnet-20240229-v1:0',"llm_type":'bedrock'}```        |
| **anthropic.claude-3-haiku-20240307-v1:0**     | ```llm_config = {"model":'anthropic.claude-3-haiku-20240307-v1:0',"llm_type":'bedrock'}```         |
| **anthropic.claude-3-opus-20240229-v1:0**      | ```llm_config = {"model":'anthropic.claude-3-opus-20240229-v1:0',"llm_type":'bedrock'}```          |
| **anthropic.claude-3-5-sonnet-20240620-v1:0**  | ```llm_config = {"model":'anthropic.claude-3-5-sonnet-20240620-v1:0',"llm_type":'bedrock'}```      |
| **cohere.command-r-v1:0**                      | ```llm_config = {"model":'cohere.command-r-v1:0',"llm_type":'bedrock'}```                          |
| **cohere.command-r-plus-v1:0**                 | ```llm_config = {"model":'cohere.command-r-plus-v1:0',"llm_type":'bedrock'}```                     |
| **meta.llama3-1-8b-instruct-v1:0**             | ```llm_config = {"model":'meta.llama3-1-8b-instruct-v1:0',"llm_type":'bedrock'}```                 |
| **meta.llama3-1-70b-instruct-v1:0**            | ```llm_config = {"model":'meta.llama3-1-70b-instruct-v1:0',"llm_type":'bedrock'}```                |
| **meta.llama3-1-405b-instruct-v1:0**           | ```llm_config = {"model":'meta.llama3-1-405b-instruct-v1:0',"llm_type":'bedrock'}```               |
| **mistral.mistral-large-2402-v1:0**            | ```llm_config = {"model":'mistral.mistral-large-2402-v1:0',"llm_type":'bedrock'}```                |
| **mistral.mistral-large-2407-v1:0**            | ```llm_config = {"model":'mistral.mistral-large-2407-v1:0',"llm_type":'bedrock'}```                |

### Ollama Supported Models:

| Type                               | Example                                                                              |
| :--------------------              | :----------------------------------------------------------                          |
| **mistral:latest**                 | ```llm_config = {"model":'mistral:latest',"llm_type":'ollama'}```                    |
| **llama3.3:latest**                | ```llm_config = {"model":'llama3.3:latest',"llm_type":'ollama'}```                   |

